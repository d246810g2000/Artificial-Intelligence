{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Episodes: 0.0 Total Reward is: 0.0\n",
      "cost time: 0.06499671936035156\n",
      "-----------------------------------------\n",
      "100 Episodes: 1.0 Total Reward is: 1.14\n",
      "cost time: 9.177297592163086\n",
      "-----------------------------------------\n",
      "100 Episodes: 2.0 Total Reward is: 2.32\n",
      "cost time: 18.70869541168213\n",
      "-----------------------------------------\n",
      "100 Episodes: 3.0 Total Reward is: 3.76\n",
      "cost time: 92.50523018836975\n",
      "-----------------------------------------\n",
      "100 Episodes: 4.0 Total Reward is: 5.5\n",
      "cost time: 160.23234939575195\n",
      "-----------------------------------------\n",
      "100 Episodes: 5.0 Total Reward is: 7.77\n",
      "cost time: 237.16179656982422\n",
      "-----------------------------------------\n",
      "100 Episodes: 6.0 Total Reward is: 10.59\n",
      "cost time: 318.02202463150024\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Dropout\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "class DQN():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.replayMemory = deque() # 初始化記憶體\n",
    "\n",
    "        self.OBSERVE = OBSERVE # 觀察階段\n",
    "        self.EXPLORE = EXPLORE # 探索階段\n",
    "        self.TRAIN = TRAIN # 訓練階段\n",
    "        self.EPISODE = EPISODE\n",
    "        self.STEP = STEP\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lr = learning_rate \n",
    "        self.epsilon = initial_epsilon \n",
    "        self.epsilon_min = final_epsilon \n",
    "        self.epsilon_decay = epsilon_decay # 遞減率\n",
    "\n",
    "        self.batch_size = batch_size  # 每次更新時從memory裡取多少記憶出來\n",
    "        self.memory_size = memory_size  # 記憶上限\n",
    "        self.learn_steps = learn_steps  # 用來控制什麼時候學習\n",
    "        self.replace_target_iter = replace_target_iter  # 更換 target net 的步數\n",
    "\n",
    "        self.action_dim = self.env.action_space.n # action 的維度\n",
    "        self.state_dim = self.env.observation_space.shape[0] # state 的維度\n",
    "           \n",
    "        self.evaluate_model = self.create_network() # 訓練模型\n",
    "        self.target_model = self.create_network() # 用來預測 Q(S,A) 的目標模型\n",
    "\n",
    "    def create_network(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(units = 128, \n",
    "                        input_dim = self.state_dim,\n",
    "                        kernel_initializer = 'random_normal',\n",
    "                        activation = \"relu\"))\n",
    "        model.add(Dense(units = 128,\n",
    "                        kernel_initializer = 'random_normal',\n",
    "                        activation = \"relu\"))\n",
    "        model.add(Dense(units = 128,\n",
    "                        kernel_initializer = 'random_normal',\n",
    "                        activation = \"relu\"))\n",
    "        model.add(Dense(units = self.action_dim,\n",
    "                        kernel_initializer = 'uniform',\n",
    "                        activation = \"linear\"))\n",
    "\n",
    "        model.compile(loss = \"mean_squared_error\",\n",
    "                      optimizer = tf.train.AdamOptimizer(self.lr)) \n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # 若learn_steps超過觀察階段則開始遞減\n",
    "        if self.epsilon > self.epsilon_min and self.learn_steps > OBSERVE: \n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        # action若小於epsilon則隨機抽取，否則取最高分數的action\n",
    "        action = None\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.evaluate_model.predict(state)[0])\n",
    "        return action\n",
    "    \n",
    "    def train_Q_network(self):  \n",
    "        #每replace_target_iter次更新一次target_net的參數\n",
    "        if self.learn_steps % self.replace_target_iter == 0:\n",
    "            self.target_model.set_weights(self.evaluate_model.get_weights())\n",
    "\n",
    "        # Step 1: 從記憶體裡隨機抽取樣本\n",
    "        minibatch = random.sample(self.replayMemory, self.batch_size)\n",
    "\n",
    "        # Step 2: 計算y\n",
    "        update_intput = np.zeros((self.batch_size, self.state_dim))\n",
    "        update_target = np.zeros((self.batch_size, self.action_dim))\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            state, action, reward, next_state, done = minibatch[i]\n",
    "            action = np.where(action == np.max(action)) # 找最好的action\n",
    "            target = self.evaluate_model.predict(state)[0]  \n",
    "            target_Qhat = self.target_model.predict(next_state)[0]             \n",
    "            \n",
    "            # 更新 Q(S,A)\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else :\n",
    "                target[action] = reward + self.gamma * np.max(target_Qhat)\n",
    "            \n",
    "            # 儲存 input & output\n",
    "            update_intput[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        # 訓練 evaluate model\n",
    "        self.evaluate_model.fit(update_intput, update_target, batch_size = self.batch_size, epochs = 1, verbose = 0)\n",
    "              \n",
    "    def percieve(self, state , action, reward, next_state, done, episode):\n",
    "\n",
    "        if len(self.replayMemory) == self.memory_size:\n",
    "            self.replayMemory.popleft()\n",
    "            \n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        \n",
    "        self.replayMemory.append((state, one_hot_action, reward, next_state, done))\n",
    "        \n",
    "        if len(self.replayMemory) > batch_size and (self.learn_steps % 4 == 0) and (self.learn_steps > OBSERVE):\n",
    "            self.train_Q_network()\n",
    "        \n",
    "        self.learn_steps += 1\n",
    "\n",
    "    def plot_reward(self,Episodes , Reward):\n",
    "        plt.plot(Episodes, Reward, marker = 'o')\n",
    "        plt.title('Avg Reward of last 100 episodes')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.show()     \n",
    "        \n",
    "# Hyper Parameters\n",
    "OBSERVE = 45000\n",
    "EXPLORE = 900000 \n",
    "TRAIN = 24000000\n",
    "EPISODE = 5000\n",
    "STEP = int((OBSERVE + EXPLORE + TRAIN)/ EPISODE)\n",
    "gamma = 0.99\n",
    "learning_rate = 0.00025\n",
    "initial_epsilon = 1.0\n",
    "final_epsilon = 0.1\n",
    "epsilon_decay = (initial_epsilon - final_epsilon) / EXPLORE \n",
    "batch_size = 32  \n",
    "memory_size = 200000  \n",
    "learn_steps = 0  \n",
    "replace_target_iter = 2500 \n",
    "\n",
    "def main():\n",
    "    env = gym.make('Breakout-ram-v0')\n",
    "    agent = DQN(env)\n",
    "    Reward = []\n",
    "    Episodes = []\n",
    "    cost_time = []\n",
    "    Total_reward = []\n",
    "    \n",
    "    for episode in range(1, EPISODE + 1):\n",
    "        tStart = time.time() # 計時開始\n",
    "\n",
    "        total_reward = 0\n",
    "        total_reward_100 = 0\n",
    "\n",
    "        state = env.reset().reshape(1, 128)\n",
    "        for step in range(STEP):\n",
    "            action = agent.get_action(state) \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = next_state.reshape(1, 128)\n",
    "            total_reward += reward\n",
    "            agent.percieve(state, action, reward, next_state, done, episode)\n",
    "            state = next_state\n",
    "\n",
    "            # 若結束遊戲則跳出\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        Total_reward.append(total_reward)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            total_reward_100 = sum(Total_reward[episode-100:episode])\n",
    "            Episodes.append(episode)\n",
    "            Reward.append(total_reward_100 / 100)\n",
    "\n",
    "            print('100 Episodes:', int(episode/100), 'Avg Reward of last 100 episodes:', total_reward_100/100)\n",
    "\n",
    "        tEnd = time.time() # 計時結束\n",
    "        cost_time.append(tEnd - tStart)\n",
    "        if episode % 100 == 0:\n",
    "            print('cost time:', sum(cost_time))\n",
    "            print('---------------------------------------------------------')\n",
    "            \n",
    "    agent.plot_reward(Episodes, Reward)        \n",
    "    print(f'The average training time per episode is {np.mean(cost_time):.2f} seconds.')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
