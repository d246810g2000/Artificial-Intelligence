{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def choose_action(state, q_table, action_space, epsilon):\n",
    "    if np.random.random_sample() < epsilon:  # random select an action with probability epsilon\n",
    "        return action_space.sample()  # random select an action\n",
    "    else:  # Choose an action according to the Q table\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "\n",
    "def get_state(observation, n_buckets):\n",
    "    state = [0] * len(observation)\n",
    "    state_bounds = [(-1.2,0.6),(-0.07,0.07)]\n",
    "    \n",
    "    #position\n",
    "    p = (state_bounds[0][1] - state_bounds[0][0]) /n_buckets[0]\n",
    "\n",
    "    #velocity\n",
    "    v = (state_bounds[1][1] - state_bounds[1][0]) /n_buckets[1]\n",
    "    \n",
    "    s1 = (observation[0] -state_bounds[0][0]) / p\n",
    "    s1 = math.ceil(s1)\n",
    "    s2 = (observation[1] -state_bounds[1][0]) / v\n",
    "    s2 = math.ceil(s2)\n",
    "    \n",
    "    state[0] = state[0] + s1 - 1\n",
    "    state[1] = state[1] + s2 - 1\n",
    "    \n",
    "    return tuple(state)\n",
    "\n",
    "\n",
    "def init_qtable():\n",
    "    n_buckets = (10, 10) \n",
    "    n_actions = env.action_space.n  # Number of actions\n",
    "    q_table = np.zeros(n_buckets + (n_actions,))\n",
    "\n",
    "    return n_buckets, n_actions, q_table\n",
    "\n",
    "\n",
    "def get_epsilon(i):\n",
    "    epsilon = 1\n",
    "    epsilon_min = 0.005\n",
    "    epsilon_decay = (epsilon - epsilon_min) / 200*(i+1)\n",
    "    epsilon -= epsilon_decay\n",
    "    return max(epsilon_min, epsilon)                                          \n",
    "\n",
    "\n",
    "def get_lr(i):\n",
    "    lr = 0.1\n",
    "    return lr\n",
    "\n",
    "\n",
    "def get_gamma(i):\n",
    "    gamma = 0.99\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "0  successes in  100 episodes, avg = -200.0\n",
      "2  successes in  200 episodes, avg = -199.99\n",
      "2  successes in  300 episodes, avg = -199.99333333333334\n",
      "29 successes in  400 episodes, avg = -199.07\n",
      "49 successes in  500 episodes, avg = -198.328\n",
      "70 successes in  600 episodes, avg = -198.11166666666668\n",
      "79 successes in  700 episodes, avg = -197.98\n",
      "102 successes in  800 episodes, avg = -197.16625\n",
      "153 successes in  900 episodes, avg = -196.12666666666667\n",
      "191 successes in 1000 episodes, avg = -194.326\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "env = gym.make(\"MountainCar-v0\")  # Select an environment\n",
    "\n",
    "\n",
    "sum_rewards = 0\n",
    "n_success = 0\n",
    "f = open('mountaincar_qtable_log.txt', 'w')\n",
    "\n",
    "n_buckets, n_actions, q_table = init_qtable()\n",
    "\n",
    "# Q-learning\n",
    "for i_episode in range(1000):\n",
    "    total_reward = 0\n",
    "    epsilon = get_epsilon(i_episode)\n",
    "    lr = get_lr(i_episode)\n",
    "    observation = env.reset()  # reset the environment and return the default observation\n",
    "    state = get_state(observation, n_buckets)\n",
    "\n",
    "    for t in range(1000):\n",
    "        # env.render()  # Show the environment in GUI\n",
    "\n",
    "        action = choose_action(state, q_table, env.action_space, epsilon)\n",
    "        observation, reward, done, info = env.step(action)  # Take an action on current environment\n",
    "        total_reward += reward\n",
    "        sum_rewards += reward\n",
    "\n",
    "        next_state = get_state(observation, n_buckets)\n",
    "\n",
    "        q_next_max = np.amax(q_table[next_state])  # The expected max reward for next state\n",
    "        q_table[state + (action,)] += lr * (reward + get_gamma(i_episode) *\n",
    "                                            q_next_max - q_table[state + (action,)])  # Update Q table\n",
    "        state = next_state\n",
    "        if done:  # Episode terminate\n",
    "            break\n",
    "\n",
    "    if total_reward > -200:\n",
    "        n_success += 1\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        avg_rewards = sum_rewards / (i_episode + 1)\n",
    "        log = '{:<2d} successes in {:4d} episodes, avg = {}'.format(n_success, i_episode + 1, avg_rewards)\n",
    "        print(log)\n",
    "        f.write(log + '\\n')\n",
    "\n",
    "#     print('Episode {:4d}, total rewards {}'.format(i_episode + 1, total_reward))\n",
    "\n",
    "f.close()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
