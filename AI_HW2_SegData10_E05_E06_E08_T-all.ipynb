{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(data):\n",
    "    d1 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 0 :\n",
    "            d1.append(i)\n",
    "    data_1 = data.iloc[d1,:].reset_index(drop = True)\n",
    "\n",
    "    d2 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 1 :\n",
    "            d2.append(i)    \n",
    "    data_2 = data.iloc[d2,:].reset_index(drop = True)\n",
    "\n",
    "    d3 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 3 :\n",
    "            d3.append(i)\n",
    "    data_3 = data.iloc[d3,:].reset_index(drop = True)\n",
    "\n",
    "    d4 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 7 :\n",
    "            d4.append(i)\n",
    "    data_4 = data.iloc[d4,:].reset_index(drop = True)\n",
    "\n",
    "    d5 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 11 :\n",
    "            d5.append(i)\n",
    "    data_5 = data.iloc[d5,:].reset_index(drop = True)\n",
    "    \n",
    "    d6 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 14 :\n",
    "            d6.append(i)\n",
    "    data_6 = data.iloc[d6,:].reset_index(drop = True)\n",
    "    \n",
    "    d7 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 18 :\n",
    "            d7.append(i)\n",
    "    data_7 = data.iloc[d7,:].reset_index(drop = True)\n",
    "    \n",
    "    d8 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 21 :\n",
    "            d8.append(i)\n",
    "    data_8 = data.iloc[d8,:].reset_index(drop = True)\n",
    "   \n",
    "    d9 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 25 :\n",
    "            d9.append(i)\n",
    "    data_9 = data.iloc[d9,:].reset_index(drop = True)\n",
    "    \n",
    "    d10 = []\n",
    "    for i in range(len(data)):\n",
    "        if data['Webpage'][i] == 29 :\n",
    "            d10.append(i)\n",
    "    data_10 = data.iloc[d10,:].reset_index(drop = True)\n",
    "\n",
    "    return data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_data(data):\n",
    "    len_max = max(len(data_1),len(data_2),len(data_3),len(data_4),len(data_5),\\\n",
    "                  len(data_6), len(data_7), len(data_8), len(data_9), len(data_10))\n",
    "    \n",
    "    for i in range(len_max):\n",
    "        if len(data_1) < len_max:\n",
    "            for j in range(len(data_1),len_max):\n",
    "                data_1.loc[j] = np.nan\n",
    "\n",
    "    for i in range(len_max):\n",
    "        if len(data_2) < len_max:\n",
    "            for j in range(len(data_2),len_max):\n",
    "                data_2.loc[j] = np.nan\n",
    "\n",
    "    for i in range(len_max):\n",
    "        if len(data_3) < len_max:\n",
    "            for j in range(len(data_3),len_max):\n",
    "                data_3.loc[j] = np.nan\n",
    "\n",
    "    for i in range(len_max):\n",
    "        if len(data_4) < len_max:\n",
    "            for j in range(len(data_4),len_max):\n",
    "                data_4.loc[j] = np.nan\n",
    "\n",
    "    for i in range(len_max):\n",
    "        if len(data_5) < len_max:\n",
    "            for j in range(len(data_5),len_max):\n",
    "                data_5.loc[j] = np.nan\n",
    "                \n",
    "    for i in range(len_max):\n",
    "        if len(data_6) < len_max:\n",
    "            for j in range(len(data_6),len_max):\n",
    "                data_6.loc[j] = np.nan\n",
    "\n",
    "    for i in range(len_max):\n",
    "        if len(data_7) < len_max:\n",
    "            for j in range(len(data_7),len_max):\n",
    "                data_7.loc[j] = np.nan\n",
    "\n",
    "    for i in range(len_max):\n",
    "        if len(data_8) < len_max:\n",
    "            for j in range(len(data_8),len_max):\n",
    "                data_8.loc[j] = np.nan\n",
    "\n",
    "    for i in range(len_max):\n",
    "        if len(data_9) < len_max:\n",
    "            for j in range(len(data_9),len_max):\n",
    "                data_9.loc[j] = np.nan\n",
    "\n",
    "    for i in range(len_max):\n",
    "        if len(data_10) < len_max:\n",
    "            for j in range(len(data_10),len_max):\n",
    "                data_10.loc[j] = np.nan\n",
    "                \n",
    "                \n",
    "    return data_1, data_2, data_3, data_4, data_5,data_6, data_7, data_8, data_9, data_10, len_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(data, data_1, data_2):\n",
    "    adjust_data(data)\n",
    "    score = []\n",
    "    for b in range(len_max):\n",
    "        if pd.isnull(data_1['Content'][b]) or pd.isnull(data_2['Content'][b]):\n",
    "            if pd.isnull(data_1['Content'][b]) and pd.isnull(data_2['Content'][b]):\n",
    "                score.append(-0.2)\n",
    "            else:\n",
    "                score.append(-0.1)\n",
    "        else:\n",
    "            if data_1['Content'][b] == data_2['Content'][b] :\n",
    "                score.append(6.5)\n",
    "            else:\n",
    "                if data_1['ContentId'][b] == data_2['ContentId'][b] :\n",
    "                    score.append(2)\n",
    "                    if data_1['TypeSetId'][b] == data_2['TypeSetId'][b] :\n",
    "                        score.append(1)\n",
    "                        if data_1['PTypeSetId'][b] == data_2['PTypeSetId'][b] :\n",
    "                            score.append(0.5)\n",
    "                            if data_1['PathId'][b] == data_2['PathId'][b] :\n",
    "                                score.append(2)\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)        \n",
    "                            else:\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)       \n",
    "                        else:\n",
    "                            if data_1['PathId'][b] == data_2['PathId'][b] :\n",
    "                                score.append(2)\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)   \n",
    "                            else:\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)   \n",
    "                    else:\n",
    "                        if data_1['PTypeSetId'][b] == data_2['PTypeSetId'][b] :\n",
    "                            score.append(0.5)\n",
    "                            if data_1['PathId'][b] == data_2['PathId'][b] :\n",
    "                                score.append(2)\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)    \n",
    "                            else:\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                        else:\n",
    "                            if data_1['PathId'][b] == data_2['PathId'][b] :\n",
    "                                score.append(2)\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                            else:\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                else:\n",
    "                    if data_1['TypeSetId'][b] == data_2['TypeSetId'][b] :\n",
    "                        score.append(1)\n",
    "                        if data_1['PTypeSetId'][b] == data_2['PTypeSetId'][b] :\n",
    "                            score.append(0.5)\n",
    "                            if data_1['PathId'][b] == data_2['PathId'][b] :\n",
    "                                score.append(2)\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                        else:\n",
    "                            if data_1['PathId'][b] == data_2['PathId'][b] :\n",
    "                                score.append(2)\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                            else:\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                    else:\n",
    "                        if data_1['PTypeSetId'][b] == data_2['PTypeSetId'][b] :\n",
    "                            score.append(0.5)\n",
    "                            if data_1['PathId'][b] == data_2['PathId'][b] :\n",
    "                                score.append(2)\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                            else:\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                        else:\n",
    "                            if data_1['PathId'][b] == data_2['PathId'][b] :\n",
    "                                score.append(2)\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "                            else:\n",
    "                                if data_1['SimTECId'][b] == data_2['SimTECId'][b] :\n",
    "                                    score.append(1)\n",
    "    return sum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore():\n",
    "    score = []\n",
    "    for x in range(len(Best)-1):\n",
    "        score.append(get_score(data.loc[dict['data'+str(i)]].reset_index(drop=True), Best[x], possibleSegs[k]))\n",
    "    return sum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_higtestScore():\n",
    "    score = []\n",
    "    for s in range(len(Best)-1):\n",
    "        score.append(get_score(data.loc[dict['data'+str(i)]].reset_index(drop=True), Best[s], Best[len(Best)-1]))\n",
    "    return sum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2csv(Best):\n",
    "    nan = np.full(len(Best[0]['Webpage']), np.nan)\n",
    "    l = [nan]*len(Best)\n",
    "    for q in range(len(Best)):\n",
    "        if np.isnan(Best[q]['Webpage']).sum() != len(Best[q]):\n",
    "            idx = Best[q]['Webpage'].index[np.where(np.isnan(Best[q]['Webpage']) == False)[0]][0]\n",
    "            if Best[q]['Webpage'][idx] == 0:\n",
    "                l[0] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 1:\n",
    "                l[1] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 3:\n",
    "                l[2] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 7:\n",
    "                l[3] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 11:\n",
    "                l[4] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 14:\n",
    "                l[5] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 18:\n",
    "                l[6] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 21:\n",
    "                l[7] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 25:\n",
    "                l[8] = Best[q][\"LeafIndex\"]\n",
    "            else :\n",
    "                l[9] = Best[q][\"LeafIndex\"]\n",
    "\n",
    "    dl = pd.DataFrame(l,index=['0', '1', '3', '7', '11', '14', '18', '21', '25', '29'])\n",
    "    dl.to_csv('C:/Users/User/Documents/人工智慧【Artificial Intelligence】/作業/HW2/Output/SegData10/T{:02}-10.csv'.format(t), mode='a', index=True, header = False)\n",
    "    with open('C:/Users/User/Documents/人工智慧【Artificial Intelligence】/作業/HW2/Output/SegData10/T{:02}-10.csv'.format(t),'a') as file:\n",
    "        writer = csv.writer(file)    \n",
    "        writer.writerow([' ','score: ' + str(allscore[i-1])])\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-all-10\n",
    "cost = []\n",
    "mean_score = [] \n",
    "for t in range(1,42):\n",
    "    tStart = time.time()\n",
    "    # 讀檔\n",
    "    data = pd.read_csv('C:/Users/User/Documents/人工智慧【Artificial Intelligence】/作業/HW2/SegData10/T{:02}-10.txt'.format(t), sep=\"\\t\")\n",
    "    data.columns = ['Webpage', 'LeafIndex', 'ContentId', 'TypeSetId', 'PTypeSetId', 'PathId', 'SimTECId', 'Content']\n",
    "\n",
    "    # 切割組別\n",
    "    a = [0]\n",
    "    for i in range(len(data)-1):\n",
    "        if data['Webpage'][i] > data['Webpage'][i+1]:\n",
    "            a.append(i+1)\n",
    "    a.append(len(data))\n",
    "\n",
    "    dict = {}\n",
    "    for i in range(1,len(a)):\n",
    "        dict['data%d' % i] = [i for i in range(a[i-1],a[i])]\n",
    "\n",
    "    allscore = []\n",
    "    for i in range(1,len(a)):\n",
    "        data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10 = combine_data(data.loc[dict['data'+str(i)]].reset_index(drop=True))\n",
    "        data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10, len_max = adjust_data(data.loc[dict['data'+str(i)]].reset_index(drop=True))\n",
    "        orginal = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
    "        r = []\n",
    "        for s in range(len(orginal)):\n",
    "            r.append(np.isnan(orginal[s]['Webpage']).sum())\n",
    "        rank = sorted(r, reverse = True)\n",
    "\n",
    "        idx = np.argsort(r)\n",
    "        orgSeg = []\n",
    "        for s in range(len(orginal)):\n",
    "            orgSeg.append(orginal[idx[s]])\n",
    "\n",
    "        alignSeg = [orgSeg[0].copy()]\n",
    "        Best = [orgSeg[0].copy()]\n",
    "        \n",
    "        score = []\n",
    "        for s in range(1,len(orgSeg)):\n",
    "            alignSeg.append(orgSeg[s].copy())\n",
    "            Best.append(orgSeg[s].copy())\n",
    "            higtestScore = get_higtestScore()\n",
    "            climb = True\n",
    "            possibleSegs = []\n",
    "            possibleSegs.insert(0, alignSeg[s].copy())\n",
    "            while(climb):\n",
    "                climb = False\n",
    "                for j in range(len(orgSeg[0])-1):\n",
    "                    if pd.isnull(alignSeg[s]['LeafIndex'][j]) == False and pd.isnull(alignSeg[s]['LeafIndex'][j+1]) :\n",
    "                        alignSeg[s][j:j+2] = alignSeg[s][j:j+2].shift(1).copy()\n",
    "                        possibleSegs.insert(0, alignSeg[s].copy())\n",
    "                for k in range(len(possibleSegs)):\n",
    "                    if higtestScore < getScore():\n",
    "                        alignSeg[s] = possibleSegs[k].copy()\n",
    "                        Best[s] = alignSeg[s].copy()\n",
    "                        higtestScore = getScore()\n",
    "                        climb = True\n",
    "                        \n",
    "            score.append(higtestScore)\n",
    "        allscore.append(sum(score))\n",
    "        \n",
    "        #轉成csv檔\n",
    "        data2csv(Best)\n",
    "\n",
    "    mean_score.append(np.mean(allscore))\n",
    "    tEnd = time.time()\n",
    "    cost.append(tEnd - tStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2csv(Best):\n",
    "    nan = np.full(len(Best[0]['Webpage']), np.nan)\n",
    "    l = [nan]*len(Best)\n",
    "    for q in range(len(Best)):\n",
    "        if np.isnan(Best[q]['Webpage']).sum() != len(Best[q]):\n",
    "            idx = Best[q]['Webpage'].index[np.where(np.isnan(Best[q]['Webpage']) == False)[0]][0]\n",
    "            if Best[q]['Webpage'][idx] == 0:\n",
    "                l[0] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 1:\n",
    "                l[1] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 3:\n",
    "                l[2] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 7:\n",
    "                l[3] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 11:\n",
    "                l[4] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 14:\n",
    "                l[5] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 18:\n",
    "                l[6] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 21:\n",
    "                l[7] = Best[q][\"LeafIndex\"]\n",
    "            elif Best[q]['Webpage'][idx] == 25:\n",
    "                l[8] = Best[q][\"LeafIndex\"]\n",
    "            else :\n",
    "                l[9] = Best[q][\"LeafIndex\"]\n",
    "\n",
    "    dl = pd.DataFrame(l,index=['0', '1', '3', '7', '11', '14', '18', '21', '25', '29'])\n",
    "    dl.to_csv('C:/Users/User/Documents/人工智慧【Artificial Intelligence】/作業/HW2/Output/SegData10/E{:02}-10.csv'.format(t), mode='a', index=True, header = False)\n",
    "    with open('C:/Users/User/Documents/人工智慧【Artificial Intelligence】/作業/HW2/Output/SegData10/E{:02}-10.csv'.format(t),'a') as file:\n",
    "        writer = csv.writer(file)    \n",
    "        writer.writerow([' ','score: ' + str(allscore[i-1])])\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E05-10, E06-10\n",
    "cost = []\n",
    "mean_score = [] \n",
    "for t in range(5,7):\n",
    "    tStart = time.time()\n",
    "    # 讀檔\n",
    "    data = pd.read_csv('C:/Users/User/Documents/人工智慧【Artificial Intelligence】/作業/HW2/SegData10/E{:02}-10.txt'.format(t), sep=\"\\t\")\n",
    "    data.columns = ['Webpage', 'LeafIndex', 'ContentId', 'TypeSetId', 'PTypeSetId', 'PathId', 'SimTECId', 'Content']\n",
    "\n",
    "    # 切割組別\n",
    "    a = [0]\n",
    "    for i in range(len(data)-1):\n",
    "        if data['Webpage'][i] > data['Webpage'][i+1]:\n",
    "            a.append(i+1)\n",
    "    a.append(len(data))\n",
    "\n",
    "    dict = {}\n",
    "    for i in range(1,len(a)):\n",
    "        dict['data%d' % i] = [i for i in range(a[i-1],a[i])]\n",
    "\n",
    "    allscore = []\n",
    "    for i in range(1,len(a)):\n",
    "        data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10 = combine_data(data.loc[dict['data'+str(i)]].reset_index(drop=True))\n",
    "        data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10, len_max = adjust_data(data.loc[dict['data'+str(i)]].reset_index(drop=True))\n",
    "        orginal = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
    "        r = []\n",
    "        for s in range(len(orginal)):\n",
    "            r.append(np.isnan(orginal[s]['Webpage']).sum())\n",
    "        rank = sorted(r, reverse = True)\n",
    "\n",
    "        idx = np.argsort(r)\n",
    "        orgSeg = []\n",
    "        for s in range(len(orginal)):\n",
    "            orgSeg.append(orginal[idx[s]])\n",
    "\n",
    "        alignSeg = [orgSeg[0].copy()]\n",
    "        Best = [orgSeg[0].copy()]\n",
    "        \n",
    "        score = []\n",
    "        for s in range(1,len(orgSeg)):\n",
    "            alignSeg.append(orgSeg[s].copy())\n",
    "            Best.append(orgSeg[s].copy())\n",
    "            higtestScore = get_higtestScore()\n",
    "            climb = True\n",
    "            possibleSegs = []\n",
    "            possibleSegs.insert(0, alignSeg[s].copy())\n",
    "            while(climb):\n",
    "                climb = False\n",
    "                for j in range(len(orgSeg[0])-1):\n",
    "                    if pd.isnull(alignSeg[s]['LeafIndex'][j]) == False and pd.isnull(alignSeg[s]['LeafIndex'][j+1]) :\n",
    "                        alignSeg[s][j:j+2] = alignSeg[s][j:j+2].shift(1).copy()\n",
    "                        possibleSegs.insert(0, alignSeg[s].copy())\n",
    "                for k in range(len(possibleSegs)):\n",
    "                    if higtestScore < getScore():\n",
    "                        alignSeg[s] = possibleSegs[k].copy()\n",
    "                        Best[s] = alignSeg[s].copy()\n",
    "                        higtestScore = getScore()\n",
    "                        climb = True\n",
    "                        \n",
    "            score.append(higtestScore)\n",
    "        allscore.append(sum(score))\n",
    "        \n",
    "        #轉成csv檔\n",
    "        data2csv(Best)\n",
    "\n",
    "    mean_score.append(np.mean(allscore))\n",
    "    tEnd = time.time()\n",
    "    cost.append(tEnd - tStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E08-10\n",
    "cost = []\n",
    "mean_score = [] \n",
    "for t in range(8,9):\n",
    "    tStart = time.time()\n",
    "    # 讀檔\n",
    "    data = pd.read_csv('C:/Users/User/Documents/人工智慧【Artificial Intelligence】/作業/HW2/SegData10/E{:02}-10.txt'.format(t), sep=\"\\t\")\n",
    "    data.columns = ['Webpage', 'LeafIndex', 'ContentId', 'TypeSetId', 'PTypeSetId', 'PathId', 'SimTECId', 'Content']\n",
    "\n",
    "    # 切割組別\n",
    "    a = [0]\n",
    "    for i in range(len(data)-1):\n",
    "        if data['Webpage'][i] > data['Webpage'][i+1]:\n",
    "            a.append(i+1)\n",
    "    a.append(len(data))\n",
    "\n",
    "    dict = {}\n",
    "    for i in range(1,len(a)):\n",
    "        dict['data%d' % i] = [i for i in range(a[i-1],a[i])]\n",
    "\n",
    "    allscore = []\n",
    "    for i in range(1,len(a)):\n",
    "        data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10 = combine_data(data.loc[dict['data'+str(i)]].reset_index(drop=True))\n",
    "        data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10, len_max = adjust_data(data.loc[dict['data'+str(i)]].reset_index(drop=True))\n",
    "        orginal = [data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10]\n",
    "        r = []\n",
    "        for s in range(len(orginal)):\n",
    "            r.append(np.isnan(orginal[s]['Webpage']).sum())\n",
    "        rank = sorted(r, reverse = True)\n",
    "\n",
    "        idx = np.argsort(r)\n",
    "        orgSeg = []\n",
    "        for s in range(len(orginal)):\n",
    "            orgSeg.append(orginal[idx[s]])\n",
    "\n",
    "        alignSeg = [orgSeg[0].copy()]\n",
    "        Best = [orgSeg[0].copy()]\n",
    "        \n",
    "        score = []\n",
    "        for s in range(1,len(orgSeg)):\n",
    "            alignSeg.append(orgSeg[s].copy())\n",
    "            Best.append(orgSeg[s].copy())\n",
    "            higtestScore = get_higtestScore()\n",
    "            climb = True\n",
    "            possibleSegs = []\n",
    "            possibleSegs.insert(0, alignSeg[s].copy())\n",
    "            while(climb):\n",
    "                climb = False\n",
    "                for j in range(len(orgSeg[0])-1):\n",
    "                    if pd.isnull(alignSeg[s]['LeafIndex'][j]) == False and pd.isnull(alignSeg[s]['LeafIndex'][j+1]) :\n",
    "                        alignSeg[s][j:j+2] = alignSeg[s][j:j+2].shift(1).copy()\n",
    "                        possibleSegs.insert(0, alignSeg[s].copy())\n",
    "                for k in range(len(possibleSegs)):\n",
    "                    if higtestScore < getScore():\n",
    "                        alignSeg[s] = possibleSegs[k].copy()\n",
    "                        Best[s] = alignSeg[s].copy()\n",
    "                        higtestScore = getScore()\n",
    "                        climb = True\n",
    "                        \n",
    "            score.append(higtestScore)\n",
    "        allscore.append(sum(score))\n",
    "        \n",
    "        #轉成csv檔\n",
    "        data2csv(Best)\n",
    "\n",
    "    mean_score.append(np.mean(allscore))\n",
    "    tEnd = time.time()\n",
    "    cost.append(tEnd - tStart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
